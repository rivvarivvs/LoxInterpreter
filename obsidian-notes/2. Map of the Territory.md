### 2.1 The Parts of a Language
- Compilers are not the same between themselves, but they all employ similar strategies.
- Metaphor of seeing the network of paths an implementation may choose as a mountain climb.
	- You start at the bottom with **raw source text**
	- At every following stage, it analyses the program and transform it to some higher-level representation
	- From there we take the high-level representation and transform it into lower level forms of itself so we can execute its instructions on the CPU
- The first step of treating the source code is **SCANNING** (or lexing)
	- It takes in the linear stream of characters and turns them into tokens. Some are simpler, some are more complex and some characters don't even mean anything (such as the whitespace)
- The second step is the **PARSING**
	- This step takes the tokens and form a tree structure that mirror the nested nature of the grammar - they're called *parse tree* or *abstract syntax tree*.
	- The parser job is also to let us know when we have syntax errors.
- The first two steps are present in every implementation, from here on out different languages can have different steps. At this point, we just have the basic structure of how the different are structured. 
- Most languages here do the **BINDING** step: for each identifier found, they try to find where they are defined. Note that scope becomes important in this step.
- If the language in question is statically typed, this is where the **TYPE CHECK** is done. 
- Here we're at the top of the metaphorical mountain. Since we have some new information about the code, we need to store it and that can be done in multiple ways
	- In most cases, it gets stored right back at the tree as *attributes*, which are nodes that get added after the parsing step.
	- In other cases, it gets stored on a separate structure such as a lookup table, off the side. If so, we'll call this a *symbol table* with the identifiers as keys
	- There's another technique that the author will mention in a later stage of the book.
- Brief section, mentioning the evolution of compilers. In the past, when they were simpler, they were separated into *front end* and *back end*. As they evolved, they started having a *middle end* which is the part we'll examine next
----
- This intermediate step only objective is to organize the gathered data into a optimised way, so that the final stages will be simpler.
- The front end is a stage that is specific to the source language the program was written in. The back end is done with the final architecture the program will run in mind.
- Between them, the middle end acts as an interface that isn't too tied up with the source code or the destination forms. This allows us to write just one front end that will match with the IR and then one back end for every target architecture.
---
- At this step, we know what the source code wants to achieve. With this knowledge in hand, we can swap out parts of it that are inefficient with more efficient ones. 
	- One example is doing the arithmetic beforehand and substituting the operation with the final value
- When we do all the optimisations that we want to do it's time to produce machine code. This is the **back end**.
	- At this point we have to decide if we want to produce instructions for a real CPU or for a virtual one. The first one is fast but generating it is a lot of work.
	- It's also difficult to generate instructions for a real CPU since there are many architectures available.
	- To get around this, some people started generating *p-code*, code for a virtual machine that is portable, which nowadays gets called *bytecode*. They're basically a dense, binary encoding of the language's low level operations.
- If your compiler produces bytecode, you still need to translate that into machine code.
	- You can either write a mini-compiler for each target arch or you can write a virtual machine that emulates a hypothetical chip that supports your virtual architecture at runtime.
	- It is slower, but you trade speed for simplicity and portability.
- When we run the code that we have, sometimes the compilers provide some services while the program is running - those are called the **runtime**. 

--- 

### 2.2 Shortcuts and Alternate Routes
The last chapter covered all the routes a compiler can take. Some languages use all of them, but some take shortcuts.

#### 2.2.1 Single-pass compilers
- These compilers interleave parsing, analysis and code generations so that they produce output code directly in the parser.
	- This frees them of allocating syntax trees or other IRs.
	- These were built back when memory was so precious that a compiler couldn't even hold an entire source file in memory.

#### 2.2.2 Tree-walk interpreters
- These interpreters run the code right after parsing it to a tree.
	- To achieve this, they traverse the tree a branch and leaf at a time.
	- It's a slow implementation but also a simple one. It's usually used for learning purposes and simple languages. 
	- The first interpreter of this book will work like this.

#### 2.2.3 Transpilers
- *Transpilers* our *source-to-source compilers* are compilers that treat other source languages as their intermediate representations. This way you can use the already existing compiling tools for that language.
	- Nowadays, transpilers work on higher level languages, mainly targeting JavaScript so that they can run their code on the browser.
- On a transpiler, the scanner and parser work just like other compilers. Then depending on the language similarities, they might skip analysis and optimisation completely. Then, they move on to code generation. 
- After this, you produce correct source code in the target language, instead of binary language like machine code.

#### 2.2.4 Just-in-time compilation
- This technique compiles the code to the machine code that the user machine supports after the program is loaded. 
	- More sophisticated JITs insert profiling hooks into the generated code to see which regions are most performance critical and what data is flowing there. Over time, they recompile those hot paths with more advanced optimizations.

---

### 2.3 Compilers and Interpreters

- What's the difference between a compiler and an interpreter?

- **Compiling** is an implementation technique that involves translating a source language to some other, be it bytecode or machine or even transpiling it to another high-level language.
- When we say that a lang implementation is a *compiler* we just mean that it translates the source code but doesn't run it.
- When we say that an implementation is an *interpreter*, we mean that it translates the source language and also executes it.
- An implementation can be either and can also be both.

![[Pasted image 20230723162643.png]]

---

